# RAG Application with MilvusDB and Ollama

A production-ready, end-to-end Retrieval-Augmented Generation (RAG) application built with Python, using MilvusDB for vector storage and Ollama for language model inference.

## ğŸŒŸ Features

- ğŸ“„ **Multi-format Document Support**: TXT, PDF, DOCX, and Markdown
- ğŸ” **Efficient Vector Search**: Powered by MilvusDB with IVF_FLAT indexing
- ğŸ¤– **Local LLM Integration**: Uses Ollama for privacy-focused inference
- ğŸ’¬ **Interactive Chat Interface**: Command-line chat with streaming responses
- ğŸ¯ **Flexible Querying**: Single query or interactive modes
- âš™ï¸ **Easy Configuration**: Environment-based settings
- ğŸ“¦ **Installable Package**: Install via pip with CLI commands
- ğŸ§ª **Unit Tests**: Comprehensive test coverage

## ğŸ“ Project Structure

```
rag_application/
â”œâ”€â”€ src/rag_app/              # Main application package
â”‚   â”œâ”€â”€ core/                 # Core functionality
â”‚   â”‚   â”œâ”€â”€ embedding_manager.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py
â”‚   â”‚   â”œâ”€â”€ milvus_manager.py
â”‚   â”‚   â””â”€â”€ rag_pipeline.py
â”‚   â”œâ”€â”€ scripts/              # CLI scripts
â”‚   â”‚   â”œâ”€â”€ ingest.py
â”‚   â”‚   â””â”€â”€ query.py
â”‚   â”œâ”€â”€ utils/                # Utilities
â”‚   â””â”€â”€ config.py             # Configuration
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ examples/                 # Usage examples
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ setup_guide.md
â”‚   â”œâ”€â”€ USAGE.md
â”‚   â””â”€â”€ PROJECT_STRUCTURE.md
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ setup.py                  # Package setup
```

## ğŸš€ Quick Start

### Prerequisites

1. **Python 3.8+**
2. **MilvusDB** (via Docker)
3. **Ollama** with a model installed

### Installation

```bash
# Clone the repository
git clone <repository-url>
cd rag_application

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install the package
pip install -e .
```

### Setup Services

**Start MilvusDB:**
```bash
docker run -d --name milvus_standalone \
  -p 19530:19530 -p 9091:9091 \
  milvusdb/milvus:latest
```

**Start Ollama:**
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model
ollama pull llama2

# Start server
ollama serve
```

### Configuration

```bash
cp .env.example .env
# Edit .env with your settings
```

### Usage

**Ingest Documents:**
```bash
# Create data directory and add documents
mkdir data
# Add your PDF, TXT, DOCX, or MD files

# Ingest documents
rag-ingest ./data/
```

**Query the System:**
```bash
# Interactive chat
rag-query

# Single query
rag-query --query "What is machine learning?"
```

## ğŸ“š Documentation

- **[Quick Start](QUICKSTART.md)**: Get started in 5 minutes
- **[Milvus Setup](docs/MILVUS_SETUP.md)**: Milvus installation and troubleshooting
- **[Project Structure](docs/PROJECT_STRUCTURE.md)**: Codebase organization

## ğŸ”§ Configuration Options

Key environment variables in `.env`:

```env
# Milvus
MILVUS_HOST=localhost
MILVUS_PORT=19530
COLLECTION_NAME=rag_documents

# Embeddings
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2

# Processing
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K=5
```

## ğŸ’» Programmatic Usage

```python
from rag_app import EmbeddingManager, MilvusManager, RAGPipeline

# Initialize components
embedding_manager = EmbeddingManager()
milvus_manager = MilvusManager()
milvus_manager.connect()
milvus_manager.load_collection()

# Create RAG pipeline
rag = RAGPipeline(embedding_manager, milvus_manager)

# Query
result = rag.query("What is AI?", top_k=5)
print(result['response'])
```

## ğŸ§ª Testing

```bash
# Run all tests
python -m pytest tests/

# Run with coverage
python -m pytest --cov=src/rag_app tests/
```

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Documents  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Document        â”‚
â”‚ Processor       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Embedding       â”‚
â”‚ Generator       â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MilvusDB        â”‚
â”‚ (Vector Store)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ RAG Pipeline    â”‚â”€â”€â”€â”€â–¶â”‚   Ollama    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Response      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ› ï¸ Development

```bash
# Install in development mode
pip install -e .

# Run tests
python -m pytest tests/

# Format code
black src/

# Type checking
mypy src/
```

## ğŸ“¦ Dependencies

- `pymilvus`: Milvus Python SDK
- `sentence-transformers`: Embedding generation
- `ollama`: Ollama Python client
- `pypdf`: PDF processing
- `python-docx`: DOCX processing
- `langchain`: Document utilities

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Acknowledgments

- [MilvusDB](https://milvus.io/) for vector storage
- [Ollama](https://ollama.ai/) for local LLM inference
- [Sentence-Transformers](https://www.sbert.net/) for embeddings
- [LangChain](https://python.langchain.com/) for document processing

## ğŸ“ Support

- **Documentation**: See `docs/` directory
- **Issues**: GitHub Issues
- **Discussions**: GitHub Discussions

## ğŸ—ºï¸ Roadmap

- [ ] Web interface
- [ ] Conversation history
- [ ] Multiple collection support
- [ ] Advanced filtering
- [ ] Batch processing optimization
- [ ] Docker Compose setup
- [ ] API server

---

**Built with â¤ï¸ for the AI community**